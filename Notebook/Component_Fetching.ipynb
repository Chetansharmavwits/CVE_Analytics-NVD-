{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents -->\n",
    "## Cell 1. Fetching NVD Data from NVD API with missing components.\n",
    "## Cell 2. Convert Json File to Excel File.\n",
    "## Cell 3. Script which gives same result as 1st but it also includes missing component which is not found on NVD.\n",
    "## cell 4. Script for converting json to excel, then read excel and match with main repository file and fetch component name and product name from NVD which are found then append in main repository other wise creat missing component file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching NVD Data from NVD API with missing components.\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "# API_URL = \"https://services.nvd.nist.gov/rest/json/cves/2.0\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def generate_dynamic_url(keyword, severity):\n",
    "    API_URL = f\"https://services.nvd.nist.gov/rest/json/cves/2.0?keywordSearch={keyword}&cvssV3Severity={severity}\"\n",
    "    print(f\"* Generated API URL: {API_URL}\")\n",
    "    return API_URL\n",
    "\n",
    "def search_nvd(keyword,severity,api_key):\n",
    "    API_URL = generate_dynamic_url(keyword, severity)\n",
    "  \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}'\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'resultsPerPage': 200,  \n",
    "        'startIndex': 0\n",
    "    }\n",
    "\n",
    "    all_cves = []\n",
    "\n",
    "    while True:\n",
    "        print(f\" * Making request to: {API_URL} with params: {params}\")\n",
    "        response = requests.get(API_URL, params=params, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"* API Response for {keyword} with severity {severity}: {data}\")\n",
    "        \n",
    "            if 'vulnerabilities' in data:\n",
    "                cves = data['vulnerabilities']\n",
    "                print(f\"* Found {len(cves)} CVEs for {keyword}\")\n",
    "\n",
    "                if len(cves) == 0:\n",
    "                   print(f\"** No CVEs found for {keyword} with severity {severity}\")\n",
    "                \n",
    "                for cve in cves:\n",
    "                    cve_id = cve['cve']['id']\n",
    "                    descriptions = cve['cve']['descriptions']\n",
    "                    description = next((d['value'] for d in descriptions if d['lang'] == 'en'), \"No description available\")\n",
    "            \n",
    "                    if 'cvssMetricV30' in cve['cve']['metrics']:\n",
    "                        for metric in cve['cve']['metrics']['cvssMetricV30']:\n",
    "                            cvss_v30 = metric.get('cvssData', {})\n",
    "                            print(f\"* CVSS 3.0 for {cve_id}: {cvss_v30}\")\n",
    "                            print(f\"* Appending CVE {cve_id} for {severity} severity (v3.0).\")\n",
    "                            if cvss_v30:\n",
    "                                cvss_v30_base_score = cvss_v30.get('baseScore', 0)\n",
    "                                cvss_v30_severity = cvss_v30.get('baseSeverity', \"\")\n",
    "                                all_cves.append({\n",
    "                                                'Component Name': keyword,\n",
    "                                                'CVE ID': cve_id,\n",
    "                                                'Descriptions': description,\n",
    "                                                'CVSS Version': \"3.0\",\n",
    "                                                'Severity': cvss_v30_severity,\n",
    "                                                'Base Score': cvss_v30_base_score\n",
    "                                            })\n",
    "                        \n",
    "                    if 'cvssMetricV31' in cve['cve']['metrics']:\n",
    "                        for metric in cve['cve']['metrics']['cvssMetricV31']:\n",
    "                            cvss_v31 = metric.get('cvssData', {})\n",
    "                            print(f\"* CVSS 3.1 for {cve_id}: {cvss_v31}\")\n",
    "                            print(f\"* Appending CVE {cve_id} for {severity} severity (v3.1).\")\n",
    "                            if cvss_v31:\n",
    "                                cvss_v31_base_score = cvss_v31.get('baseScore', 0)\n",
    "                                cvss_v31_severity = cvss_v31.get('baseSeverity', \"\")\n",
    "                                all_cves.append({\n",
    "                                            'Component Name': keyword,\n",
    "                                            'CVE ID': cve_id,\n",
    "                                            'Descriptions': description,\n",
    "                                            'CVSS Version': \"3.1\",\n",
    "                                            'Severity': cvss_v31_severity,\n",
    "                                            'Base Score': cvss_v31_base_score\n",
    "                                             })\n",
    "\n",
    "\n",
    "                if len(cves) < 200:\n",
    "                    print(\"Less than 200 CVEs found, breaking the loop.\")\n",
    "                    break\n",
    "                params['startIndex'] += 200\n",
    "\n",
    "            else:\n",
    "                print(f\"** No CVE data found for this {keyword}.\")\n",
    "                break\n",
    "\n",
    "        elif response.status_code == 403:\n",
    "            logger.warning(\"*** Received 403 error. Sleeping for 30 seconds and retrying...\")\n",
    "            time.sleep(30)\n",
    "            continue  # Retrying\n",
    "        else:\n",
    "            logger.error(f\"** Failed to retrieve data for {keyword}. HTTP Status Code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return all_cves\n",
    "\n",
    "def process_excel(input_file, severities ,api_key):\n",
    "    ''' \n",
    "    Processing the Component file\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    start_timestamp = datetime.now()\n",
    "    print(f\"Script started at: {start_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    output_file_name = 'NVD_CVE_Data_WITH_3.0_AND_3.1.xlsx'\n",
    "    \n",
    "    if os.path.exists(output_file_name):\n",
    "        os.remove(output_file_name)\n",
    "        print(f\"Deleted existing file: {output_file_name}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "    df = pd.read_excel(input_file, engine='openpyxl')\n",
    "    print(f\"Excel data read successfully. Number of components: {len(df)}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "   \n",
    "    if 'Component Name' not in df.columns:\n",
    "        print(\"'Component Name' column is missing in the input file.\")\n",
    "        print(\"---------------------------------------------\")\n",
    "        return pd.DataFrame()\n",
    "    component_names = df['Component Name'].dropna().unique()\n",
    "    all_cve_data = []\n",
    "\n",
    "    total_components = len(component_names)\n",
    "\n",
    "    for idx, component in enumerate(component_names):\n",
    "        # # Break the loop if processed the 5 components\n",
    "        # if idx >= max_components_to_process:\n",
    "        #     print(f\"Processed {max_components_to_process} components, stopping execution.\")\n",
    "        #     break\n",
    "        print(f\"* Processing component {idx + 1}/{total_components}: {component}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "        # Fetch HIGH and CRITICAL CVE data\n",
    "        for severity in severities:\n",
    "            print(f\"Fetching {severity} severity CVEs for component: {component}\")\n",
    "            print(\"---------------------------------------------\")\n",
    "            \n",
    "            severity_cve_data = search_nvd(component, severity, api_key)\n",
    "            if severity_cve_data:\n",
    "                print(f\"Found {len(severity_cve_data)} CVEs for severity {severity}.\")\n",
    "                print(\"---------------------------------------------\")\n",
    "                all_cve_data.extend(severity_cve_data)\n",
    "            else:\n",
    "                print(f\"No CVEs found for severity {severity} for component {component}.\")\n",
    "                print(\"---------------------------------------------\")\n",
    "    \n",
    "    if all_cve_data:\n",
    "        cve_df = pd.DataFrame(all_cve_data)\n",
    "        output_file_name = 'NVD_CVE_Data_WITH_3.0_AND_3.1.xlsx'\n",
    "        cve_df.to_excel(output_file_name, index=False, engine='openpyxl')\n",
    "        print(f\"Data saved to {output_file_name}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(\"---------------------------------------------\")\n",
    "    else:\n",
    "        print(\"No CVE data collected. Output file not created.\")\n",
    "        cve_df = pd.DataFrame()\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_timestamp = datetime.now()\n",
    "    total_time = timedelta(seconds=(end_time - start_time))\n",
    "\n",
    "    print(f\"Script ended at: {end_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Total processing time: {total_time}\")\n",
    "    print(\"------XXXX------XXXX---------XXXXXX-----\")\n",
    "    return cve_df\n",
    "    \n",
    "    \n",
    "\n",
    "def main(input_file, api_key):\n",
    "    '''\n",
    "    calling function\n",
    "    '''\n",
    "    severity = [\"HIGH\",\"CRITICAL\"]\n",
    "    cve_df = process_excel(input_file, severity, api_key)\n",
    "    return cve_df.head()\n",
    "\n",
    "\n",
    "input_file = '/Users/f0tj5ln/code/CVE_Search/reference datas/component name.xlsx'  \n",
    "api_key = \"585bc7f5-b4b5-4e85-a9c3-40f21f9d7e98\"\n",
    "\n",
    "main(input_file, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for converting Json file to Excel file\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "with open('/Users/f0tj5ln/code/CVE_Search/reference datas/Components 3.json', 'r', encoding='utf-16') as file:\n",
    "    data = json.load(file)\n",
    "# Access the 'Type' attribute from each dictionary in the list\n",
    "products = data[1].get('Products')\n",
    "data_list = []\n",
    "for product in products:\n",
    "    module = product.get('module')\n",
    "    file_name = product.get('info').get('filename')\n",
    "    file_without_ext = file_name.replace(\".dll\", \"\")\n",
    "    company_name = product.get('info').get('companyName')\n",
    "    product_description = product.get('info').get('productDescription')\n",
    "    product_version = product.get('info').get('productVersion')\n",
    "    product_name = product.get('info').get('productName')\n",
    "\n",
    "    data_list.append({\n",
    "        'Module': module,\n",
    "        'File Name': file_name,\n",
    "        'File Without Extension': file_without_ext,\n",
    "        'Company Name': company_name,\n",
    "        'Product Name': product_name,\n",
    "        'Product Description': product_description,\n",
    "        'Product Version': product_version\n",
    "        \n",
    "    })\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Drop duplicate rows based on specific columns\n",
    "df_unique = df.drop_duplicates(subset=['Module','File Name', 'File Without Extension', 'Product Description'])\n",
    "\n",
    "\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_file_path = '/Users/f0tj5ln/code/CVE_Search/extracted_json_data2.xlsx'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(output_file_path):\n",
    "    # Delete the file\n",
    "    os.remove(output_file_path)\n",
    "    print(f\"Previous file {output_file_path} deleted.\")\n",
    "\n",
    "# Save the new DataFrame to the same location\n",
    "df_unique.to_excel(output_file_path, index=False, engine='openpyxl')\n",
    "print(f\"New data successfully saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script which gives same result as 1st but it also includes missing component which is not found on NVD\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import shutil\n",
    "import re\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "\n",
    "# API_URL = \"https://services.nvd.nist.gov/rest/json/cves/2.0\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def generate_dynamic_url(keyword, severity):\n",
    "    API_URL = f\"https://services.nvd.nist.gov/rest/json/cves/2.0?keywordSearch={keyword}&cvssV3Severity={severity}\"\n",
    "    print(f\"* Generated API URL: {API_URL}\")\n",
    "    return API_URL\n",
    "\n",
    "def search_nvd(keyword, severity, api_key):\n",
    "    API_URL = generate_dynamic_url(keyword, severity)\n",
    "  \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}'\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'resultsPerPage': 2000,  \n",
    "        'startIndex': 0\n",
    "    }\n",
    "\n",
    "    all_cves = []\n",
    "\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    while True:\n",
    "        print(f\" * Making request to: {API_URL} with params: {params}\")\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = session.get(API_URL, params=params, headers=headers)\n",
    "                break  # Exit the loop if successful\n",
    "            except (requests.exceptions.ProxyError, requests.exceptions.ConnectionError, requests.exceptions.ChunkedEncodingError) as e:\n",
    "                    print(f\"*** Connection error encountered: {e}. Attempt {attempt + 1} of {max_retries}.\")\n",
    "                # Retry logic with a maximum of 3 attempts\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"Retrying...\")\n",
    "                time.sleep(30)  # Wait for 30 seconds before retrying\n",
    "            else:\n",
    "                print(\"Max retries reached. Moving to the next component.\")\n",
    "                time.sleep(30)\n",
    "                continue  # Retry the same request after sleep\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"* API Response for {keyword} with severity {severity}\")\n",
    "        \n",
    "            if 'vulnerabilities' in data:\n",
    "                cves = data['vulnerabilities']\n",
    "                print(f\"* Found {len(cves)} CVEs for {keyword}\")\n",
    "\n",
    "                if len(cves) == 0:\n",
    "                   print(f\"** No CVEs found for {keyword} with severity {severity}\")\n",
    "                \n",
    "                for cve in cves:\n",
    "                    cve_id = cve['cve']['id']\n",
    "                    descriptions = cve['cve']['descriptions']\n",
    "                    description = next((d['value'] for d in descriptions if d['lang'] == 'en'), \"No description available\")\n",
    "                    if 'cvssMetricV30' in cve['cve']['metrics']:\n",
    "                        for metric in cve['cve']['metrics']['cvssMetricV30']:\n",
    "                            cvss_v30_source = metric.get('source', \"\")\n",
    "                            cvss_v30_type = metric.get('type', \"\")\n",
    "                            cvss_v30 = metric.get('cvssData', {})\n",
    "                            if cvss_v30:\n",
    "                                cvss_v30_base_score = cvss_v30.get('baseScore', 0)\n",
    "                                cvss_v30_severity = cvss_v30.get('baseSeverity', \"\")\n",
    "                                cvss_v30_vectorString = cvss_v30.get('vectorString', \"\")\n",
    "                                all_cves.append({\n",
    "                                                'Component Name': keyword,\n",
    "                                                'CVE ID': cve_id,\n",
    "                                                'Descriptions': description,\n",
    "                                                'Type': cvss_v30_type,\n",
    "                                                'Source': cvss_v30_source,\n",
    "                                                'Vector String': cvss_v30_vectorString,\n",
    "                                                'CVSS Version': \"3.0\",\n",
    "                                                'Severity': cvss_v30_severity,\n",
    "                                                'Base Score': cvss_v30_base_score\n",
    "                                            })\n",
    "                        \n",
    "                    if 'cvssMetricV31' in cve['cve']['metrics']:\n",
    "                        for metric in cve['cve']['metrics']['cvssMetricV31']:\n",
    "                            cvss_v31_source = metric.get('source', \"\")\n",
    "                            cvss_v31_type = metric.get('type', \"\")\n",
    "                            cvss_v31 = metric.get('cvssData', {})\n",
    "                            if cvss_v31:\n",
    "                                cvss_v31_base_score = cvss_v31.get('baseScore', 0)\n",
    "                                cvss_v31_severity = cvss_v31.get('baseSeverity', \"\")\n",
    "                                cvss_v31_vectorString = cvss_v31.get('vectorString', \"\")\n",
    "                                all_cves.append({\n",
    "                                                'Component Name': keyword,\n",
    "                                                'CVE ID': cve_id,\n",
    "                                                'Descriptions': description,\n",
    "                                                'Type': cvss_v31_type,\n",
    "                                                'Source': cvss_v31_source,\n",
    "                                                'Vector String': cvss_v31_vectorString,\n",
    "                                                'CVSS Version': \"3.1\",\n",
    "                                                'Severity': cvss_v31_severity,\n",
    "                                                'Base Score': cvss_v31_base_score\n",
    "                                             })\n",
    "\n",
    "\n",
    "                if len(cves) < 200:\n",
    "                    print(\"Less than 200 CVEs found, breaking the loop.\")\n",
    "                    break\n",
    "                params['startIndex'] += 2000\n",
    "\n",
    "            else:\n",
    "                print(f\"** No CVE data found for this {keyword}.\")\n",
    "                break\n",
    "\n",
    "        elif response.status_code == 403:\n",
    "            logger.warning(\"*** Received 403 error. Sleeping for 30 seconds and retrying...\")\n",
    "            time.sleep(30)\n",
    "            continue  # Retry the same request after sleep\n",
    "        else:\n",
    "            logger.error(f\"** Failed to retrieve data for {keyword}. HTTP Status Code: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    return all_cves\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_excel(input_file, NVD_file, Missing_component_file, severities ,api_key, archive_folder):\n",
    "    ''' \n",
    "    Processing the Component file\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    start_timestamp = datetime.now()\n",
    "    print(f\"Script started at: {start_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    # output_file_name = [NVD_File, Missing_components_file]\n",
    "\n",
    "    # for file_name in output_file_name:\n",
    "    #     if os.path.exists(file_name):\n",
    "    #         os.remove(file_name)\n",
    "    #         print(f\"deleted existing file: {file_name}\")\n",
    "    #         print(\"---------------------------------------------\")\n",
    "    #     else:\n",
    "    #         print(f\"File does not exist: {file_name}. Skipping deletion.\")\n",
    "\n",
    "\n",
    "    df = pd.read_excel(input_file, engine='openpyxl')\n",
    "    print(f\"Excel data read successfully. Number of components: {len(df)}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "   \n",
    "    if 'Component Name' not in df.columns:\n",
    "        print(\"'Component Name' column is missing in the input file.\")\n",
    "        print(\"---------------------------------------------\")\n",
    "        return pd.DataFrame()\n",
    "    component_names = df['Component Name'].dropna().unique()\n",
    "    all_cve_data = []\n",
    "    missing_components = []  # To store components not found on NVD\n",
    "\n",
    "    # max_components_to_process = 5\n",
    "    total_components = len(component_names)\n",
    "\n",
    "    for idx, component in enumerate(component_names):\n",
    "        # Break the loop if we have processed the first 10 components\n",
    "        # if idx >= max_components_to_process:\n",
    "        #     print(f\"Processed {max_components_to_process} components, stopping further execution.\")\n",
    "        #     break\n",
    "        component_found = False  # Flag to check if component data is found\n",
    "\n",
    "        print(f\"* Processing component {idx + 1}/{total_components}: {component}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "        # Fetch HIGH and CRITICAL CVE data\n",
    "        for severity in severities:\n",
    "            print(f\"Fetching {severity} severity CVEs for component: {component}\")\n",
    "            print(\"---------------------------------------------\")\n",
    "            \n",
    "            severity_cve_data = search_nvd(component, severity, api_key)\n",
    "            if severity_cve_data:\n",
    "                print(f\"Found {len(severity_cve_data)} CVEs for severity {severity}.\")\n",
    "                print(\"---------------------------------------------\")\n",
    "                all_cve_data.extend(severity_cve_data)\n",
    "                component_found = True  # Mark as found if data is returned\n",
    "\n",
    "        if not component_found:\n",
    "            print(f\"No CVEs found for component: {component}\")\n",
    "            missing_components.append({'Component Name': component})\n",
    "\n",
    "    \n",
    "    if all_cve_data:\n",
    "        cve_df = pd.DataFrame(all_cve_data)\n",
    "        output_file_name = 'new_sheet_NVD.xlsx'\n",
    "        cve_df.to_excel(output_file_name, index=False, engine='openpyxl')\n",
    "        print(f\"Data saved to {output_file_name}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(\"---------------------------------------------\")\n",
    "    else:\n",
    "        print(\"No CVE data collected. Output file not created.\")\n",
    "        cve_df = pd.DataFrame()\n",
    "\n",
    "    # Save missing components to Excel\n",
    "    if missing_components:\n",
    "        missing_df = pd.DataFrame(missing_components)\n",
    "        missing_output_file = 'Missing_Components.xlsx'\n",
    "        missing_df.to_excel(missing_output_file, index=False, engine='openpyxl')\n",
    "        print(f\"Missing components saved to {output_file_not_found}\")\n",
    "    else:\n",
    "        print(\"No missing components found. Missing components file not created.\")\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_timestamp = datetime.now()\n",
    "    total_time = timedelta(seconds=(end_time - start_time))\n",
    "\n",
    "    print(f\"Script ended at: {end_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")    \n",
    "    print(f\"Total processing time: {total_time}\")\n",
    "    print(\"------XXXX------XXXX---------XXXXXX-----\")\n",
    "\n",
    "    return cve_df, missing_df\n",
    "\n",
    "\n",
    "def main(input_file, NVD_file, Missing_component_file, api_key):\n",
    "    '''\n",
    "    calling function\n",
    "    '''\n",
    "    severities = [\"HIGH\", \"CRITICAL\"]\n",
    "    cve_df, missing_component_df = process_excel(input_file, NVD_file, Missing_component_file, severities, api_key)\n",
    "    return cve_df.head(), missing_component_df.head()\n",
    "\n",
    "\n",
    "input_file = 'Gives Input file path where it reads the data'  \n",
    "NVD_file = 'Gives the output file path if exist'\n",
    "Missing_component_file = 'Give missing component file path if exist'\n",
    "api_key = \"585bc7f5-b4b5-4e85-a9c3-40f21f9d7e98\" \n",
    "\n",
    "main(input_file,NVD_file, Missing_component_file, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script for converting json to excel, then read excel and match with main repository file and fetch component name and product name from NVD which are found then append \n",
    "# in main repository other wise creat missing component file.\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "yellow = \"\\033[33m\"\n",
    "reset = \"\\033[0m\"\n",
    "green = \"\\033[32m\"\n",
    "red = \"\\033[31m\"\n",
    "cyan = \"\\033[36m\"\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "def json_to_excel(json_path:str)->pd.DataFrame:\n",
    "    '''\n",
    "    Convert Json File to Excel file.\n",
    "    '''\n",
    "    with open(json_path, 'r', encoding='utf-16') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Access the 'Type' attribute from each dictionary in the list\n",
    "    products = data[1].get('Products')\n",
    "    data_list = []\n",
    "    for product in products:\n",
    "        module = product.get('module')\n",
    "        file_name = product.get('info').get('filename')\n",
    "        # file_without_ext = file_name.replace(\".dll\", \"\")\n",
    "        company_name = product.get('info').get('companyName')\n",
    "        product_description = product.get('info').get('productDescription')\n",
    "        product_version = product.get('info').get('productVersion')\n",
    "        product_name = product.get('info').get('productName')\n",
    "\n",
    "        data_list.append({\n",
    "                            'Module': module,\n",
    "                            'File Name': file_name,\n",
    "                            'Product Name': product_name,\n",
    "                            'Product Description': product_description,\n",
    "                            'Company Name': company_name,\n",
    "                            'Product Version': product_version    \n",
    "                        })\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df_unique = df.drop_duplicates(subset=['Module','File Name', 'Product Description'])\n",
    "    return df_unique\n",
    "\n",
    "def merge_product_component(file1: pd.DataFrame, file2: str) -> pd.DataFrame:\n",
    "    df2 = pd.read_excel(file2, engine='openpyxl') \n",
    "    file_to_component = df2.set_index('File Name')['Component Name'].to_dict()\n",
    "    file1['Component Name'] = file1['File Name'].map(file_to_component)\n",
    "    output_columns_file1 = ['Module', 'File Name', 'Company Name', 'Product Name', 'Component Name', 'Product Description',  'Product Version']\n",
    "    output_columns_df2 = ['File Name', 'Component Name']\n",
    "    return file1[output_columns_file1], df2[output_columns_df2]\n",
    "\n",
    "\n",
    "def generate_dynamic_url(keyword, severity):\n",
    "    API_URL = f\"https://services.nvd.nist.gov/rest/json/cves/2.0?keywordSearch={keyword}&cvssV3Severity={severity}\"\n",
    "    return API_URL\n",
    "\n",
    "\n",
    "def search_nvd(keyword, severity, api_key):\n",
    "    API_URL = generate_dynamic_url(keyword, severity)\n",
    "  \n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {api_key}'\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'resultsPerPage': 2000,  \n",
    "        'startIndex': 0\n",
    "    }\n",
    "\n",
    "    all_cves = []\n",
    "\n",
    "    session = requests.Session()\n",
    "    retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "    session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    while True:\n",
    "        print(f\" * Making request to: {API_URL} with params: {params}\")\n",
    "        max_retries = 3\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = session.get(API_URL, params=params, headers=headers)\n",
    "                break  # Exit the loop if successful\n",
    "            except (requests.exceptions.ProxyError, requests.exceptions.ConnectionError, requests.exceptions.ChunkedEncodingError) as e:\n",
    "                    print(f\"*** Connection error encountered: {e}. Attempt {attempt + 1} of {max_retries}.\")\n",
    "                # Retry logic with a maximum of 3 attempts\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"Retrying...\")\n",
    "                time.sleep(30)  # Wait for 30 seconds before retrying\n",
    "            else:\n",
    "                print(\"Max retries reached. Moving to the next component.\")\n",
    "                time.sleep(30)\n",
    "                continue  # Retry the same request after sleep\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(f\"* API Response for {keyword} with severity {severity}\")\n",
    "        \n",
    "            if 'vulnerabilities' in data:\n",
    "                cves = data['vulnerabilities']\n",
    "                print(f\"* Found {len(cves)} CVEs for {keyword}\")\n",
    "\n",
    "                if len(cves) == 0:\n",
    "                   print(f\"{red}** No CVEs found for {keyword} with severity {severity}{reset}\")\n",
    "                \n",
    "                for cve in cves:\n",
    "                    cve_id = cve['cve']['id']\n",
    "                    descriptions = cve['cve']['descriptions']\n",
    "                    description = next((d['value'] for d in descriptions if d['lang'] == 'en'), \"No description available\")\n",
    "                    if 'cvssMetricV30' in cve['cve']['metrics']:\n",
    "                        for metric in cve['cve']['metrics']['cvssMetricV30']:\n",
    "                            cvss_v30_source = metric.get('source', \"\")\n",
    "                            cvss_v30_type = metric.get('type', \"\")\n",
    "                            cvss_v30 = metric.get('cvssData', {})\n",
    "                            if cvss_v30:\n",
    "                                cvss_v30_base_score = cvss_v30.get('baseScore', 0)\n",
    "                                cvss_v30_severity = cvss_v30.get('baseSeverity', \"\")\n",
    "                                cvss_v30_vectorString = cvss_v30.get('vectorString', \"\")\n",
    "                                all_cves.append({\n",
    "                                                'Component Name': keyword,\n",
    "                                                'CVE ID': cve_id,\n",
    "                                                'Descriptions': description,\n",
    "                                                'Type': cvss_v30_type,\n",
    "                                                'Source': cvss_v30_source,\n",
    "                                                'Vector String': cvss_v30_vectorString,\n",
    "                                                'CVSS Version': \"3.0\",\n",
    "                                                'Severity': cvss_v30_severity,\n",
    "                                                'Base Score': cvss_v30_base_score\n",
    "                                            })\n",
    "                            \n",
    "                        \n",
    "                    if 'cvssMetricV31' in cve['cve']['metrics']:\n",
    "                        for metric in cve['cve']['metrics']['cvssMetricV31']:\n",
    "                            cvss_v31_source = metric.get('source', \"\")\n",
    "                            cvss_v31_type = metric.get('type', \"\")\n",
    "                            cvss_v31 = metric.get('cvssData', {})\n",
    "                            if cvss_v31:\n",
    "                                cvss_v31_base_score = cvss_v31.get('baseScore', 0)\n",
    "                                cvss_v31_severity = cvss_v31.get('baseSeverity', \"\")\n",
    "                                cvss_v31_vectorString = cvss_v31.get('vectorString', \"\")\n",
    "                                all_cves.append({\n",
    "                                                'Component Name': keyword,\n",
    "                                                'CVE ID': cve_id,\n",
    "                                                'Descriptions': description,\n",
    "                                                'Type': cvss_v31_type,\n",
    "                                                'Source': cvss_v31_source,\n",
    "                                                'Vector String': cvss_v31_vectorString,\n",
    "                                                'CVSS Version': \"3.1\",\n",
    "                                                'Severity': cvss_v31_severity,\n",
    "                                                'Base Score': cvss_v31_base_score\n",
    "                                             })\n",
    "                    \n",
    "\n",
    "\n",
    "                if len(cves) < 200:\n",
    "                    print(\"Less than 200 CVEs found, breaking the loop.\")\n",
    "                    break\n",
    "                params['startIndex'] += 2000\n",
    "\n",
    "            else:\n",
    "                print(f\"{red}** No CVE data found for this {keyword}{reset}.\")\n",
    "                break\n",
    "\n",
    "        elif response.status_code == 403:\n",
    "            logger.warning(f\"{red}*** Received 403 error. Sleeping for 30 seconds and retrying...{reset}\")\n",
    "            time.sleep(30)\n",
    "            continue  # Retry the same request after sleep\n",
    "        else:\n",
    "            logger.error(f\"{red}** Failed to retrieve data for {keyword}. HTTP Status Code: {response.status_code}{reset}\")\n",
    "            break\n",
    "\n",
    "    return all_cves\n",
    "\n",
    "def process_excel(json_file, repo_file, get_components_file, missing_components_file, severities, api_key):\n",
    "    ''' \n",
    "    Processing the Component file\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "    start_timestamp = datetime.now()\n",
    "    print(f\"Script started at: {start_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "    product_df = json_to_excel(json_file)\n",
    "    print(\"*** Step 1st Json to Excel Started\")\n",
    "\n",
    "    df,df2 = merge_product_component(product_df,repo_file)\n",
    "    print(\"*** Step 2 product and component dataframe merged\")\n",
    "    \n",
    "    if os.path.exists(get_components_file):\n",
    "        os.remove(get_components_file)\n",
    "        print(f\"deleted existing file: {get_components_file}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "    else:\n",
    "        print(f\"File does not exist: {get_components_file}. Skipping deletion.\")\n",
    "    \n",
    "    if os.path.exists(missing_components_file):\n",
    "        os.remove(missing_components_file)\n",
    "        print(f\"deleted existing file: {missing_components_file}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "    else:\n",
    "        print(f\"File does not exist: {missing_components_file}. Skipping deletion.\")\n",
    "\n",
    "\n",
    "    print(f\"Excel data read successfully. Number of components: {len(df)}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "   \n",
    "    if 'Product Name' not in df.columns or 'Component Name' not in df.columns:\n",
    "        print(\"Required columns ('Product Name' or 'Component Name') are missing in the input file.\")\n",
    "        print(\"---------------------------------------------\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df['Query Name'] = df['Component Name'].fillna(df['Product Name']).replace(\"Â®\", \"\").replace(\"  \", \" \")  # Use Component Name if available, else Product Name\n",
    "    unique_queries = df['Query Name'].drop_duplicates().tolist()\n",
    "    print(f\"Number of unique queries to fetch: {len(unique_queries)}\")\n",
    "\n",
    "    all_cve_data = []\n",
    "    missing_components = []\n",
    "    processed_queries = set()  # To track already processed queries\n",
    "    \n",
    "    max_components_to_process = 30\n",
    "    total_components = len(unique_queries)\n",
    "    print(total_components)\n",
    "    for idx, query_name in enumerate(unique_queries):\n",
    "        # # Break the loop if we have processed the first 5 components\n",
    "        if idx >= max_components_to_process:\n",
    "            print(f\"Processed {max_components_to_process} components, stopping further execution.\")\n",
    "            break\n",
    "        if query_name in processed_queries:\n",
    "            continue  # Skip already processed queries\n",
    "        component_found = False\n",
    "        print(f\"* Processing query {idx + 1}/{max_components_to_process}: {cyan}{query_name}{reset}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "       \n",
    "        # Fetch HIGH and CRITICAL CVE data\n",
    "        for severity in severities:\n",
    "            print(f\"{yellow}Fetching {severity} severity CVEs for component --> {query_name}{reset}\")\n",
    "            print(\"---------------------------------------------\")\n",
    "            \n",
    "            severity_cve_data = search_nvd(query_name, severity, api_key)\n",
    "            if severity_cve_data:\n",
    "                print(f\"{green}Found {len(severity_cve_data)} CVE IDs for severity {severity}.{reset}\")\n",
    "                print(\"---------------------------------------------\")\n",
    "                all_cve_data.extend(severity_cve_data)\n",
    "                component_found = True  # Mark as found if data is returned\n",
    "            \n",
    "        if component_found:\n",
    "        # Append query_name and corresponding file names to file2 if not already present\n",
    "            matching_rows = df[df['Query Name'] == query_name]\n",
    "\n",
    "            for _, row in matching_rows.iterrows():\n",
    "                file_name = row['File Name']\n",
    "                component_name = row['Query Name']\n",
    "\n",
    "                # Check if file name already exists in file2\n",
    "                if file_name not in df2['File Name'].values:\n",
    "                    # Append new row to file2\n",
    "                    new_row = pd.DataFrame({'File Name': [file_name], 'Component Name': [component_name]})\n",
    "                    df2 = pd.concat([df2, new_row], ignore_index=True)\n",
    "    \n",
    "    \n",
    "        else:\n",
    "            # Add all rows corresponding to the missing query\n",
    "            matching_rows = df[df['Query Name'] == query_name].to_dict('records')\n",
    "            missing_components.extend(matching_rows)\n",
    "            print(f\"{query_name} not found in NVD database.\")\n",
    "            print(\"---------------------------------------------\")\n",
    "\n",
    "        processed_queries.add(query_name)\n",
    "\n",
    "    df2.drop_duplicates(subset=['File Name'], inplace=True)  # Ensure file names are unique\n",
    "    df2.to_excel(repo_file, index=False, engine='openpyxl')  # Overwrite the existing file2\n",
    "    print(f\"{green}Updated file2 saved to {repo_file}{reset}\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    \n",
    "    if all_cve_data:\n",
    "        cve_df = pd.DataFrame(all_cve_data)\n",
    "        output_file_name = get_components_file\n",
    "        cve_df.to_excel(output_file_name, index=False, engine='openpyxl')\n",
    "        print(f\"{green}Data saved to {output_file_name}{reset}\")\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(\"---------------------------------------------\")\n",
    "    else:\n",
    "        print(\"No CVE data collected. Output file not created.\")\n",
    "        cve_df = pd.DataFrame()\n",
    "\n",
    "    # Save missing components to Excel\n",
    "    if missing_components:\n",
    "        missing_df = pd.DataFrame(missing_components).drop_duplicates(subset=['Module', 'File Name'])\n",
    "        missing_output_file = missing_components_file\n",
    "        missing_df.to_excel(missing_output_file, index=False, engine='openpyxl')\n",
    "        print(f\"{red}Missing components saved to {missing_output_file}{reset}\")\n",
    "    else:\n",
    "        print(\"No missing components found. Missing components file not created.\")\n",
    "    print(\"---------------------------------------------\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_timestamp = datetime.now()\n",
    "    total_time = timedelta(seconds=(end_time - start_time))\n",
    "\n",
    "    print(f\"Script ended at: {end_timestamp.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Total processing time: {total_time}\")\n",
    "    print(\"------XXXX------XXXX---------XXXXXX-----\")\n",
    "    return cve_df, missing_df\n",
    "    \n",
    "    \n",
    "\n",
    "def main(json_file, repo_file, api_key, get_components_file, missing_components_file):\n",
    "    '''\n",
    "    calling function\n",
    "    '''\n",
    "    severities = [\"HIGH\", \"CRITICAL\"]\n",
    "    cve_df, missing_df = process_excel(json_file, repo_file, get_components_file, missing_components_file, severities, api_key)\n",
    "    return cve_df.head(), missing_df.head()\n",
    "\n",
    "\n",
    "json_file = '/Users/f0tj5ln/code/CVE_Search/reference datas/Components 3.json'\n",
    "repo_file = \"/Users/f0tj5ln/code/CVE_Search/reference datas/Export copy.xlsx\"\n",
    "get_components_file = '/Users/f0tj5ln/code/CVE_Search/Product_data_from_Json2.xlsx'\n",
    "missing_components_file = '/Users/f0tj5ln/code/CVE_Search/Missing_Components_Product_data_from_Json2.xlsx'  \n",
    "api_key = \"585bc7f5-b4b5-4e85-a9c3-40f21f9d7e98\"\n",
    "\n",
    "main(json_file, repo_file, api_key, get_components_file, missing_components_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
